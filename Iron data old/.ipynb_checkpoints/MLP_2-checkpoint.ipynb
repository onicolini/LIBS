{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>188.195100</th>\n",
       "      <th>188.260100</th>\n",
       "      <th>188.325000</th>\n",
       "      <th>188.390000</th>\n",
       "      <th>188.455000</th>\n",
       "      <th>188.520000</th>\n",
       "      <th>188.585000</th>\n",
       "      <th>188.650000</th>\n",
       "      <th>188.714900</th>\n",
       "      <th>188.779900</th>\n",
       "      <th>...</th>\n",
       "      <th>440.300000</th>\n",
       "      <th>440.353000</th>\n",
       "      <th>440.406100</th>\n",
       "      <th>440.459200</th>\n",
       "      <th>440.512300</th>\n",
       "      <th>440.565300</th>\n",
       "      <th>440.618400</th>\n",
       "      <th>440.671400</th>\n",
       "      <th>440.724500</th>\n",
       "      <th>440.777500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582.463687</td>\n",
       "      <td>504.208566</td>\n",
       "      <td>530.709497</td>\n",
       "      <td>467.608939</td>\n",
       "      <td>591.122905</td>\n",
       "      <td>435.320298</td>\n",
       "      <td>543.597765</td>\n",
       "      <td>528.843575</td>\n",
       "      <td>556.465549</td>\n",
       "      <td>527.795158</td>\n",
       "      <td>...</td>\n",
       "      <td>3151.193669</td>\n",
       "      <td>8059.815642</td>\n",
       "      <td>12561.966480</td>\n",
       "      <td>16587.173184</td>\n",
       "      <td>16647.337058</td>\n",
       "      <td>9118.242086</td>\n",
       "      <td>3947.782123</td>\n",
       "      <td>2177.945996</td>\n",
       "      <td>1990.074488</td>\n",
       "      <td>1936.068901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>577.814035</td>\n",
       "      <td>501.428070</td>\n",
       "      <td>528.219298</td>\n",
       "      <td>464.108772</td>\n",
       "      <td>587.801754</td>\n",
       "      <td>430.540351</td>\n",
       "      <td>539.671930</td>\n",
       "      <td>523.103509</td>\n",
       "      <td>553.461404</td>\n",
       "      <td>524.222807</td>\n",
       "      <td>...</td>\n",
       "      <td>2957.377193</td>\n",
       "      <td>7532.591228</td>\n",
       "      <td>11824.056140</td>\n",
       "      <td>15650.092982</td>\n",
       "      <td>15969.352632</td>\n",
       "      <td>8896.112281</td>\n",
       "      <td>3865.568421</td>\n",
       "      <td>2129.036842</td>\n",
       "      <td>1942.419298</td>\n",
       "      <td>1889.075439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>581.423759</td>\n",
       "      <td>503.906028</td>\n",
       "      <td>530.794326</td>\n",
       "      <td>464.395390</td>\n",
       "      <td>589.487589</td>\n",
       "      <td>431.744681</td>\n",
       "      <td>540.849291</td>\n",
       "      <td>526.014184</td>\n",
       "      <td>555.927305</td>\n",
       "      <td>526.069149</td>\n",
       "      <td>...</td>\n",
       "      <td>2854.895390</td>\n",
       "      <td>7315.283688</td>\n",
       "      <td>11555.730496</td>\n",
       "      <td>15321.815603</td>\n",
       "      <td>15800.558511</td>\n",
       "      <td>8872.147163</td>\n",
       "      <td>3840.040780</td>\n",
       "      <td>2087.900709</td>\n",
       "      <td>1880.914894</td>\n",
       "      <td>1813.776596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578.072897</td>\n",
       "      <td>502.925234</td>\n",
       "      <td>528.289720</td>\n",
       "      <td>464.824299</td>\n",
       "      <td>588.863551</td>\n",
       "      <td>430.351402</td>\n",
       "      <td>538.553271</td>\n",
       "      <td>523.919626</td>\n",
       "      <td>553.955140</td>\n",
       "      <td>524.906542</td>\n",
       "      <td>...</td>\n",
       "      <td>3003.185047</td>\n",
       "      <td>7658.478505</td>\n",
       "      <td>12141.448598</td>\n",
       "      <td>16085.945794</td>\n",
       "      <td>16729.994393</td>\n",
       "      <td>9503.841121</td>\n",
       "      <td>4113.334579</td>\n",
       "      <td>2228.928972</td>\n",
       "      <td>2001.278505</td>\n",
       "      <td>1946.420561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>583.411215</td>\n",
       "      <td>505.267290</td>\n",
       "      <td>530.114019</td>\n",
       "      <td>467.000000</td>\n",
       "      <td>592.957009</td>\n",
       "      <td>433.781308</td>\n",
       "      <td>542.345794</td>\n",
       "      <td>527.211215</td>\n",
       "      <td>557.394393</td>\n",
       "      <td>528.370093</td>\n",
       "      <td>...</td>\n",
       "      <td>2774.921495</td>\n",
       "      <td>7201.949533</td>\n",
       "      <td>11434.900935</td>\n",
       "      <td>15151.863551</td>\n",
       "      <td>15868.080374</td>\n",
       "      <td>9025.482243</td>\n",
       "      <td>3888.373832</td>\n",
       "      <td>2084.327103</td>\n",
       "      <td>1842.557009</td>\n",
       "      <td>1761.603738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>580.806452</td>\n",
       "      <td>504.989247</td>\n",
       "      <td>530.980287</td>\n",
       "      <td>466.141577</td>\n",
       "      <td>591.779570</td>\n",
       "      <td>432.125448</td>\n",
       "      <td>542.329749</td>\n",
       "      <td>526.629032</td>\n",
       "      <td>556.978495</td>\n",
       "      <td>525.913978</td>\n",
       "      <td>...</td>\n",
       "      <td>2855.363799</td>\n",
       "      <td>7378.372760</td>\n",
       "      <td>11764.557348</td>\n",
       "      <td>15582.700717</td>\n",
       "      <td>16437.143369</td>\n",
       "      <td>9465.284946</td>\n",
       "      <td>4104.107527</td>\n",
       "      <td>2200.274194</td>\n",
       "      <td>1956.034050</td>\n",
       "      <td>1891.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>580.168856</td>\n",
       "      <td>502.399625</td>\n",
       "      <td>529.767355</td>\n",
       "      <td>465.416510</td>\n",
       "      <td>592.009381</td>\n",
       "      <td>430.292683</td>\n",
       "      <td>540.981238</td>\n",
       "      <td>523.521576</td>\n",
       "      <td>556.236398</td>\n",
       "      <td>525.386492</td>\n",
       "      <td>...</td>\n",
       "      <td>2733.628518</td>\n",
       "      <td>7118.538462</td>\n",
       "      <td>11405.639775</td>\n",
       "      <td>15141.915572</td>\n",
       "      <td>16115.547842</td>\n",
       "      <td>9358.979362</td>\n",
       "      <td>4052.003752</td>\n",
       "      <td>2145.215760</td>\n",
       "      <td>1884.500938</td>\n",
       "      <td>1808.534709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>578.602251</td>\n",
       "      <td>501.377111</td>\n",
       "      <td>527.679174</td>\n",
       "      <td>462.786116</td>\n",
       "      <td>589.510319</td>\n",
       "      <td>427.848030</td>\n",
       "      <td>538.525328</td>\n",
       "      <td>522.223265</td>\n",
       "      <td>554.397749</td>\n",
       "      <td>523.478424</td>\n",
       "      <td>...</td>\n",
       "      <td>2885.328330</td>\n",
       "      <td>7654.039400</td>\n",
       "      <td>12315.624765</td>\n",
       "      <td>16371.080675</td>\n",
       "      <td>17422.896811</td>\n",
       "      <td>10061.281426</td>\n",
       "      <td>4293.876173</td>\n",
       "      <td>2230.523452</td>\n",
       "      <td>1930.776735</td>\n",
       "      <td>1838.643527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>578.147448</td>\n",
       "      <td>501.041588</td>\n",
       "      <td>527.729679</td>\n",
       "      <td>462.417769</td>\n",
       "      <td>590.109641</td>\n",
       "      <td>428.406427</td>\n",
       "      <td>539.328922</td>\n",
       "      <td>522.826087</td>\n",
       "      <td>554.115312</td>\n",
       "      <td>522.739130</td>\n",
       "      <td>...</td>\n",
       "      <td>2746.215501</td>\n",
       "      <td>7174.523629</td>\n",
       "      <td>11497.368620</td>\n",
       "      <td>15236.415879</td>\n",
       "      <td>16183.637051</td>\n",
       "      <td>9354.826087</td>\n",
       "      <td>4037.888469</td>\n",
       "      <td>2131.748582</td>\n",
       "      <td>1859.650284</td>\n",
       "      <td>1771.839319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>578.640232</td>\n",
       "      <td>501.669246</td>\n",
       "      <td>527.361702</td>\n",
       "      <td>461.479691</td>\n",
       "      <td>589.104449</td>\n",
       "      <td>430.323017</td>\n",
       "      <td>538.415861</td>\n",
       "      <td>523.052224</td>\n",
       "      <td>554.355899</td>\n",
       "      <td>523.307544</td>\n",
       "      <td>...</td>\n",
       "      <td>2794.477756</td>\n",
       "      <td>7428.918762</td>\n",
       "      <td>11936.990329</td>\n",
       "      <td>15900.007737</td>\n",
       "      <td>16831.435203</td>\n",
       "      <td>9659.495164</td>\n",
       "      <td>4115.816248</td>\n",
       "      <td>2144.887814</td>\n",
       "      <td>1855.294004</td>\n",
       "      <td>1759.114120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>580.690939</td>\n",
       "      <td>502.902913</td>\n",
       "      <td>529.969256</td>\n",
       "      <td>465.920712</td>\n",
       "      <td>592.053398</td>\n",
       "      <td>432.177994</td>\n",
       "      <td>540.877023</td>\n",
       "      <td>525.789644</td>\n",
       "      <td>556.603560</td>\n",
       "      <td>526.211974</td>\n",
       "      <td>...</td>\n",
       "      <td>2852.478964</td>\n",
       "      <td>7552.033981</td>\n",
       "      <td>12110.386731</td>\n",
       "      <td>16103.199029</td>\n",
       "      <td>16978.679612</td>\n",
       "      <td>9730.744337</td>\n",
       "      <td>4157.765372</td>\n",
       "      <td>2180.600324</td>\n",
       "      <td>1896.729773</td>\n",
       "      <td>1805.559871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>582.007394</td>\n",
       "      <td>502.998152</td>\n",
       "      <td>528.828096</td>\n",
       "      <td>466.665434</td>\n",
       "      <td>592.670980</td>\n",
       "      <td>432.563771</td>\n",
       "      <td>541.484288</td>\n",
       "      <td>526.112754</td>\n",
       "      <td>558.534196</td>\n",
       "      <td>527.449168</td>\n",
       "      <td>...</td>\n",
       "      <td>2797.768946</td>\n",
       "      <td>7348.312384</td>\n",
       "      <td>11762.473198</td>\n",
       "      <td>15636.415896</td>\n",
       "      <td>16407.253235</td>\n",
       "      <td>9349.195933</td>\n",
       "      <td>4011.992606</td>\n",
       "      <td>2122.426987</td>\n",
       "      <td>1863.937153</td>\n",
       "      <td>1777.820702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>580.127013</td>\n",
       "      <td>502.243292</td>\n",
       "      <td>530.386404</td>\n",
       "      <td>465.373882</td>\n",
       "      <td>589.958855</td>\n",
       "      <td>431.806798</td>\n",
       "      <td>540.023256</td>\n",
       "      <td>525.862254</td>\n",
       "      <td>556.919499</td>\n",
       "      <td>526.048301</td>\n",
       "      <td>...</td>\n",
       "      <td>2736.939177</td>\n",
       "      <td>7113.152057</td>\n",
       "      <td>11315.463327</td>\n",
       "      <td>14991.463327</td>\n",
       "      <td>15707.016100</td>\n",
       "      <td>8956.186047</td>\n",
       "      <td>3873.218247</td>\n",
       "      <td>2064.656530</td>\n",
       "      <td>1816.389982</td>\n",
       "      <td>1722.996422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>580.758879</td>\n",
       "      <td>503.684112</td>\n",
       "      <td>529.242991</td>\n",
       "      <td>464.542056</td>\n",
       "      <td>592.020561</td>\n",
       "      <td>430.865421</td>\n",
       "      <td>539.188785</td>\n",
       "      <td>526.545794</td>\n",
       "      <td>555.728972</td>\n",
       "      <td>526.813084</td>\n",
       "      <td>...</td>\n",
       "      <td>2706.994393</td>\n",
       "      <td>6986.035514</td>\n",
       "      <td>11128.287850</td>\n",
       "      <td>14657.456075</td>\n",
       "      <td>15336.403738</td>\n",
       "      <td>8738.474766</td>\n",
       "      <td>3798.465421</td>\n",
       "      <td>2042.725234</td>\n",
       "      <td>1815.317757</td>\n",
       "      <td>1730.317757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>582.619850</td>\n",
       "      <td>505.994382</td>\n",
       "      <td>530.958801</td>\n",
       "      <td>468.024345</td>\n",
       "      <td>593.749064</td>\n",
       "      <td>433.823970</td>\n",
       "      <td>542.674157</td>\n",
       "      <td>527.191011</td>\n",
       "      <td>559.155431</td>\n",
       "      <td>528.604869</td>\n",
       "      <td>...</td>\n",
       "      <td>2786.468165</td>\n",
       "      <td>7283.726592</td>\n",
       "      <td>11650.413858</td>\n",
       "      <td>15390.664794</td>\n",
       "      <td>16181.397004</td>\n",
       "      <td>9256.110487</td>\n",
       "      <td>3997.657303</td>\n",
       "      <td>2139.058052</td>\n",
       "      <td>1894.638577</td>\n",
       "      <td>1817.966292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>582.135338</td>\n",
       "      <td>506.082707</td>\n",
       "      <td>531.689850</td>\n",
       "      <td>467.708647</td>\n",
       "      <td>593.842105</td>\n",
       "      <td>433.727444</td>\n",
       "      <td>543.146617</td>\n",
       "      <td>526.392857</td>\n",
       "      <td>558.043233</td>\n",
       "      <td>528.276316</td>\n",
       "      <td>...</td>\n",
       "      <td>2743.210526</td>\n",
       "      <td>7109.375940</td>\n",
       "      <td>11345.787594</td>\n",
       "      <td>14969.575188</td>\n",
       "      <td>15741.315789</td>\n",
       "      <td>9048.229323</td>\n",
       "      <td>3941.511278</td>\n",
       "      <td>2124.725564</td>\n",
       "      <td>1883.894737</td>\n",
       "      <td>1808.548872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>581.724771</td>\n",
       "      <td>503.447706</td>\n",
       "      <td>531.691743</td>\n",
       "      <td>467.000000</td>\n",
       "      <td>593.700917</td>\n",
       "      <td>432.919266</td>\n",
       "      <td>542.352294</td>\n",
       "      <td>526.240367</td>\n",
       "      <td>558.141284</td>\n",
       "      <td>527.163303</td>\n",
       "      <td>...</td>\n",
       "      <td>2831.794495</td>\n",
       "      <td>7310.855046</td>\n",
       "      <td>11657.258716</td>\n",
       "      <td>15396.566972</td>\n",
       "      <td>16059.269725</td>\n",
       "      <td>9145.385321</td>\n",
       "      <td>3956.007339</td>\n",
       "      <td>2125.150459</td>\n",
       "      <td>1889.594495</td>\n",
       "      <td>1815.390826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>579.581921</td>\n",
       "      <td>503.760829</td>\n",
       "      <td>528.762712</td>\n",
       "      <td>465.039548</td>\n",
       "      <td>591.907721</td>\n",
       "      <td>432.291902</td>\n",
       "      <td>542.467043</td>\n",
       "      <td>524.495292</td>\n",
       "      <td>554.943503</td>\n",
       "      <td>526.453861</td>\n",
       "      <td>...</td>\n",
       "      <td>2983.062147</td>\n",
       "      <td>7762.169492</td>\n",
       "      <td>12344.583804</td>\n",
       "      <td>16290.924670</td>\n",
       "      <td>16976.512241</td>\n",
       "      <td>9650.694915</td>\n",
       "      <td>4164.887006</td>\n",
       "      <td>2222.367232</td>\n",
       "      <td>1961.937853</td>\n",
       "      <td>1889.839925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>582.138196</td>\n",
       "      <td>505.184261</td>\n",
       "      <td>530.819578</td>\n",
       "      <td>467.936660</td>\n",
       "      <td>594.322457</td>\n",
       "      <td>433.130518</td>\n",
       "      <td>542.589251</td>\n",
       "      <td>527.447217</td>\n",
       "      <td>557.566219</td>\n",
       "      <td>528.257198</td>\n",
       "      <td>...</td>\n",
       "      <td>2826.071017</td>\n",
       "      <td>7446.426104</td>\n",
       "      <td>11879.452975</td>\n",
       "      <td>15743.719770</td>\n",
       "      <td>16444.291747</td>\n",
       "      <td>9310.326296</td>\n",
       "      <td>3997.328215</td>\n",
       "      <td>2111.648752</td>\n",
       "      <td>1859.090211</td>\n",
       "      <td>1771.479846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>583.412548</td>\n",
       "      <td>504.433460</td>\n",
       "      <td>530.528517</td>\n",
       "      <td>466.558935</td>\n",
       "      <td>593.057034</td>\n",
       "      <td>432.167300</td>\n",
       "      <td>542.526616</td>\n",
       "      <td>526.813688</td>\n",
       "      <td>557.532319</td>\n",
       "      <td>529.047529</td>\n",
       "      <td>...</td>\n",
       "      <td>2811.150190</td>\n",
       "      <td>7349.125475</td>\n",
       "      <td>11750.228137</td>\n",
       "      <td>15538.450570</td>\n",
       "      <td>16272.357414</td>\n",
       "      <td>9267.935361</td>\n",
       "      <td>4002.346008</td>\n",
       "      <td>2140.351711</td>\n",
       "      <td>1903.425856</td>\n",
       "      <td>1834.830798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 4094 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    188.195100  188.260100  188.325000  188.390000  188.455000  188.520000  \\\n",
       "0   582.463687  504.208566  530.709497  467.608939  591.122905  435.320298   \n",
       "1   577.814035  501.428070  528.219298  464.108772  587.801754  430.540351   \n",
       "2   581.423759  503.906028  530.794326  464.395390  589.487589  431.744681   \n",
       "3   578.072897  502.925234  528.289720  464.824299  588.863551  430.351402   \n",
       "4   583.411215  505.267290  530.114019  467.000000  592.957009  433.781308   \n",
       "5   580.806452  504.989247  530.980287  466.141577  591.779570  432.125448   \n",
       "6   580.168856  502.399625  529.767355  465.416510  592.009381  430.292683   \n",
       "7   578.602251  501.377111  527.679174  462.786116  589.510319  427.848030   \n",
       "8   578.147448  501.041588  527.729679  462.417769  590.109641  428.406427   \n",
       "9   578.640232  501.669246  527.361702  461.479691  589.104449  430.323017   \n",
       "10  580.690939  502.902913  529.969256  465.920712  592.053398  432.177994   \n",
       "11  582.007394  502.998152  528.828096  466.665434  592.670980  432.563771   \n",
       "12  580.127013  502.243292  530.386404  465.373882  589.958855  431.806798   \n",
       "13  580.758879  503.684112  529.242991  464.542056  592.020561  430.865421   \n",
       "14  582.619850  505.994382  530.958801  468.024345  593.749064  433.823970   \n",
       "15  582.135338  506.082707  531.689850  467.708647  593.842105  433.727444   \n",
       "16  581.724771  503.447706  531.691743  467.000000  593.700917  432.919266   \n",
       "17  579.581921  503.760829  528.762712  465.039548  591.907721  432.291902   \n",
       "18  582.138196  505.184261  530.819578  467.936660  594.322457  433.130518   \n",
       "19  583.412548  504.433460  530.528517  466.558935  593.057034  432.167300   \n",
       "\n",
       "    188.585000  188.650000  188.714900  188.779900  ...   440.300000  \\\n",
       "0   543.597765  528.843575  556.465549  527.795158  ...  3151.193669   \n",
       "1   539.671930  523.103509  553.461404  524.222807  ...  2957.377193   \n",
       "2   540.849291  526.014184  555.927305  526.069149  ...  2854.895390   \n",
       "3   538.553271  523.919626  553.955140  524.906542  ...  3003.185047   \n",
       "4   542.345794  527.211215  557.394393  528.370093  ...  2774.921495   \n",
       "5   542.329749  526.629032  556.978495  525.913978  ...  2855.363799   \n",
       "6   540.981238  523.521576  556.236398  525.386492  ...  2733.628518   \n",
       "7   538.525328  522.223265  554.397749  523.478424  ...  2885.328330   \n",
       "8   539.328922  522.826087  554.115312  522.739130  ...  2746.215501   \n",
       "9   538.415861  523.052224  554.355899  523.307544  ...  2794.477756   \n",
       "10  540.877023  525.789644  556.603560  526.211974  ...  2852.478964   \n",
       "11  541.484288  526.112754  558.534196  527.449168  ...  2797.768946   \n",
       "12  540.023256  525.862254  556.919499  526.048301  ...  2736.939177   \n",
       "13  539.188785  526.545794  555.728972  526.813084  ...  2706.994393   \n",
       "14  542.674157  527.191011  559.155431  528.604869  ...  2786.468165   \n",
       "15  543.146617  526.392857  558.043233  528.276316  ...  2743.210526   \n",
       "16  542.352294  526.240367  558.141284  527.163303  ...  2831.794495   \n",
       "17  542.467043  524.495292  554.943503  526.453861  ...  2983.062147   \n",
       "18  542.589251  527.447217  557.566219  528.257198  ...  2826.071017   \n",
       "19  542.526616  526.813688  557.532319  529.047529  ...  2811.150190   \n",
       "\n",
       "     440.353000    440.406100    440.459200    440.512300    440.565300  \\\n",
       "0   8059.815642  12561.966480  16587.173184  16647.337058   9118.242086   \n",
       "1   7532.591228  11824.056140  15650.092982  15969.352632   8896.112281   \n",
       "2   7315.283688  11555.730496  15321.815603  15800.558511   8872.147163   \n",
       "3   7658.478505  12141.448598  16085.945794  16729.994393   9503.841121   \n",
       "4   7201.949533  11434.900935  15151.863551  15868.080374   9025.482243   \n",
       "5   7378.372760  11764.557348  15582.700717  16437.143369   9465.284946   \n",
       "6   7118.538462  11405.639775  15141.915572  16115.547842   9358.979362   \n",
       "7   7654.039400  12315.624765  16371.080675  17422.896811  10061.281426   \n",
       "8   7174.523629  11497.368620  15236.415879  16183.637051   9354.826087   \n",
       "9   7428.918762  11936.990329  15900.007737  16831.435203   9659.495164   \n",
       "10  7552.033981  12110.386731  16103.199029  16978.679612   9730.744337   \n",
       "11  7348.312384  11762.473198  15636.415896  16407.253235   9349.195933   \n",
       "12  7113.152057  11315.463327  14991.463327  15707.016100   8956.186047   \n",
       "13  6986.035514  11128.287850  14657.456075  15336.403738   8738.474766   \n",
       "14  7283.726592  11650.413858  15390.664794  16181.397004   9256.110487   \n",
       "15  7109.375940  11345.787594  14969.575188  15741.315789   9048.229323   \n",
       "16  7310.855046  11657.258716  15396.566972  16059.269725   9145.385321   \n",
       "17  7762.169492  12344.583804  16290.924670  16976.512241   9650.694915   \n",
       "18  7446.426104  11879.452975  15743.719770  16444.291747   9310.326296   \n",
       "19  7349.125475  11750.228137  15538.450570  16272.357414   9267.935361   \n",
       "\n",
       "     440.618400   440.671400   440.724500   440.777500  \n",
       "0   3947.782123  2177.945996  1990.074488  1936.068901  \n",
       "1   3865.568421  2129.036842  1942.419298  1889.075439  \n",
       "2   3840.040780  2087.900709  1880.914894  1813.776596  \n",
       "3   4113.334579  2228.928972  2001.278505  1946.420561  \n",
       "4   3888.373832  2084.327103  1842.557009  1761.603738  \n",
       "5   4104.107527  2200.274194  1956.034050  1891.935484  \n",
       "6   4052.003752  2145.215760  1884.500938  1808.534709  \n",
       "7   4293.876173  2230.523452  1930.776735  1838.643527  \n",
       "8   4037.888469  2131.748582  1859.650284  1771.839319  \n",
       "9   4115.816248  2144.887814  1855.294004  1759.114120  \n",
       "10  4157.765372  2180.600324  1896.729773  1805.559871  \n",
       "11  4011.992606  2122.426987  1863.937153  1777.820702  \n",
       "12  3873.218247  2064.656530  1816.389982  1722.996422  \n",
       "13  3798.465421  2042.725234  1815.317757  1730.317757  \n",
       "14  3997.657303  2139.058052  1894.638577  1817.966292  \n",
       "15  3941.511278  2124.725564  1883.894737  1808.548872  \n",
       "16  3956.007339  2125.150459  1889.594495  1815.390826  \n",
       "17  4164.887006  2222.367232  1961.937853  1889.839925  \n",
       "18  3997.328215  2111.648752  1859.090211  1771.479846  \n",
       "19  4002.346008  2140.351711  1903.425856  1834.830798  \n",
       "\n",
       "[20 rows x 4094 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df = pd.read_csv('data_with_avg_intesities_reduced.csv', sep = ',')\n",
    "values = x_df.values\n",
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[ 582.463687  504.208566  530.709497 ... 2177.945996 1990.074488\n",
      "  1936.068901]\n",
      " [ 577.814035  501.42807   528.219298 ... 2129.036842 1942.419298\n",
      "  1889.075439]\n",
      " [ 581.423759  503.906028  530.794326 ... 2087.900709 1880.914894\n",
      "  1813.776596]\n",
      " ...\n",
      " [ 579.581921  503.760829  528.762712 ... 2222.367232 1961.937853\n",
      "  1889.839925]\n",
      " [ 582.138196  505.184261  530.819578 ... 2111.648752 1859.090211\n",
      "  1771.479846]\n",
      " [ 583.412548  504.43346   530.528517 ... 2140.351711 1903.425856\n",
      "  1834.830798]]\n",
      "(20, 4094)\n"
     ]
    }
   ],
   "source": [
    "print(type(values))\n",
    "print(values)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 582.463687,  504.208566,  530.709497, ..., 2177.945996,\n",
       "       1990.074488, 1936.068901])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=values[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['B16', 93.63, 2.48, 0.12, 0.13, 0.052000000000000005, 0.024,\n",
       "        0.012, 0.03],\n",
       "       ['B15', 93.55, 2.53, 0.13, 0.13, 0.09300000000000001, 0.024,\n",
       "        0.012, 0.033],\n",
       "       ['B14', 93.65, 2.49, 0.12, 0.13, 0.11, 0.024, 0.012,\n",
       "        0.036000000000000004],\n",
       "       ['B13', 91.68, 2.46, 0.19, 0.64, 1.18, 0.023, 0.27,\n",
       "        0.057999999999999996],\n",
       "       ['B12', 92.47, 2.62, 0.36, 0.83, 0.07, 0.022000000000000002,\n",
       "        0.018000000000000002, 0.049],\n",
       "       ['B11', 93.23, 2.31, 0.17, 0.55, 0.061, 0.025,\n",
       "        0.013999999999999999, 0.07200000000000001],\n",
       "       ['B10', 93.24, 2.27, 0.17, 0.55, 0.061, 0.025,\n",
       "        0.013000000000000001, 0.066],\n",
       "       ['A01', 92.94, 1.84, 0.19, 0.64, 0.42, 0.022000000000000002,\n",
       "        0.26899999999999996, 0.0024],\n",
       "       ['A02', 93.35, 1.8, 0.17, 0.48, 0.37, 0.022000000000000002,\n",
       "        0.17300000000000001, 0.0017],\n",
       "       ['A03', 94.15, 1.59, 0.14, 0.32, 0.06, 0.024, 0.016, 0.001],\n",
       "       ['A04', 93.7, 1.38, 0.2, 0.66, 0.04, 0.018000000000000002,\n",
       "        0.26899999999999996, 0.0017],\n",
       "       ['A05', 94.33, 1.63, 0.12, 0.11, 0.09, 0.019, 0.019, 0.0022],\n",
       "       ['A06', 94.51, 1.5, 0.12, 0.09, 0.04, 0.021, 0.009000000000000001,\n",
       "        0.0009],\n",
       "       ['A07', 94.01, 1.31, 0.2, 0.61, 0.07, 0.019, 0.016, 0.0046],\n",
       "       ['C17', 92.27, 2.54, 0.16, 1.13, 0.095, 0.027000000000000003,\n",
       "        0.027000000000000003, 0.0568],\n",
       "       ['C18', 92.79, 2.4, 0.21, 0.65, 0.071, 0.019, 0.265, 0.047],\n",
       "       ['C19', 91.81, 2.39, 0.2, 0.65, 1.1, 0.021, 0.267, 0.0542],\n",
       "       ['C20', 91.81, 2.42, 0.19, 0.65, 1.06, 0.021, 0.263, 0.0442],\n",
       "       ['C21', 92.37, 2.61, 0.16, 1.17, 0.062, 0.019, 0.015, 0.0504],\n",
       "       ['C22', 92.07, 4.4, 0.2, 0.14, 0.053, 0.022000000000000002, 0.012,\n",
       "        0.0468]], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y = pd.read_csv('labels_aligned.csv', sep = ',')\n",
    "concentration=df_y.values\n",
    "print(concentration.shape)\n",
    "concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B16' 2.48]\n",
      " ['B15' 2.53]\n",
      " ['B14' 2.49]\n",
      " ['B13' 2.46]\n",
      " ['B12' 2.62]\n",
      " ['B11' 2.31]\n",
      " ['B10' 2.27]\n",
      " ['A01' 1.84]\n",
      " ['A02' 1.8]\n",
      " ['A03' 1.59]\n",
      " ['A04' 1.38]\n",
      " ['A05' 1.63]\n",
      " ['A06' 1.5]\n",
      " ['A07' 1.31]\n",
      " ['C17' 2.54]\n",
      " ['C18' 2.4]\n",
      " ['C19' 2.39]\n",
      " ['C20' 2.42]\n",
      " ['C21' 2.61]\n",
      " ['C22' 4.4]]\n",
      "(20, 2)\n"
     ]
    }
   ],
   "source": [
    "y_si = concentration[:,[0,2]]\n",
    "print(y_si)\n",
    "print(y_si.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.48 2.53 2.49 2.46 2.62 2.31 2.27 1.84 1.8 1.59 1.38 1.63 1.5 1.31 2.54\n",
      " 2.4 2.39 2.42 2.61 4.4]\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "y_si = concentration[:,2]\n",
    "print(y_si)\n",
    "print(y_si.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0   1.84\n",
       "1    1.8\n",
       "2   1.59\n",
       "3   1.38\n",
       "4   1.63\n",
       "5    1.5\n",
       "6   1.31\n",
       "7   2.27\n",
       "8   2.31\n",
       "9   2.62\n",
       "10  2.46\n",
       "11  2.49\n",
       "12  2.53\n",
       "13  2.48\n",
       "14  2.54\n",
       "15   2.4\n",
       "16  2.39\n",
       "17  2.42\n",
       "18  2.61\n",
       "19   4.4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_si= pd.DataFrame(y_si)\n",
    "df_y_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4094) (16, 1)\n",
      "(4, 4094) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_df, df_y_si, test_size=0.2)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-46-be863ba47157>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-46-be863ba47157>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    from matplotlib import pyplot as pltpredictions = lm.predict(X_test)\u001b[0m\n\u001b[1;37m                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# fit a model\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predictions')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  -3.2319491008729786\n"
     ]
    }
   ],
   "source": [
    "print ('Score: ', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=4094, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown optimizer: opt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-195248b007c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m model.compile(optimizer = \"opt\", loss = root_mean_squared_error, \n\u001b[1;32m----> 9\u001b[1;33m               metrics =[\"accuracy\"])\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'config'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    766\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m                                     printable_module_name='optimizer')\n\u001b[0m\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[1;32m--> 138\u001b[1;33m                                  ': ' + class_name)\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown optimizer: opt"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=2e-3, decay=1e-3 / 200)\n",
    "#model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "model.compile(optimizer = \"opt\", loss = root_mean_squared_error, \n",
    "              metrics =[\"accuracy\"])\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(X_train, y_train, epochs=200, validation_split=0.2, batch_size=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, Reshape#, MaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers.noise import GaussianNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for the network\n",
    "DENSE = 128\n",
    "DROPOUT = 0.5\n",
    "C1_K  = 8 #Number of kernels/feature extractors for first layer\n",
    "C1_S  = 32 #Width of the convolutional mini networks\n",
    "C2_K  = 16\n",
    "C2_S  = 32\n",
    "\n",
    "activation='relu'\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "#The model\n",
    "def make_model():\n",
    "    model = Sequential()\n",
    "    #Adding a bit of GaussianNoise also works as regularization\n",
    "    model.add(GaussianNoise(0.05, input_shape=(input_dim,)))\n",
    "    #First two is number of filter + kernel size\n",
    "    model.add(Reshape((input_dim, 1)))\n",
    "    model.add(Conv1D(C1_K, (C1_S), activation=activation, border_mode=\"same\"))\n",
    "    model.add(Conv1D(C2_K, (C2_S), border_mode=\"same\", activation=activation))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(DENSE, activation=activation))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer=keras.optimizers.Adadelta(lr=0.01))#, metrics=['mean_absolute_error'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\olivier.nicolini\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\olivier.nicolini\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_1 (GaussianNo (None, 4094)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4094, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 4094, 8)           264       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 4094, 16)          4112      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 65504)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 65504)             0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 128)               8384640   \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 8,389,145\n",
      "Trainable params: 8,389,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivier.nicolini\\.conda\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(8, 32, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\olivier.nicolini\\.conda\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(16, 32, activation=\"relu\", padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16 samples, validate on 4 samples\n",
      "Epoch 1/600\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 5.5819 - val_loss: 4.0363\n",
      "Epoch 2/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 5.3376 - val_loss: 3.9618\n",
      "Epoch 3/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 5.0505 - val_loss: 3.8761\n",
      "Epoch 4/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 4.7759 - val_loss: 3.7788\n",
      "Epoch 5/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 4.4056 - val_loss: 3.6649\n",
      "Epoch 6/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 4.0462 - val_loss: 3.5372\n",
      "Epoch 7/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 3.6545 - val_loss: 3.3966\n",
      "Epoch 8/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 3.2958 - val_loss: 3.2419\n",
      "Epoch 9/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 2.9379 - val_loss: 3.0723\n",
      "Epoch 10/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2.6074 - val_loss: 2.8914\n",
      "Epoch 11/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 2.2401 - val_loss: 2.7014\n",
      "Epoch 12/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 1.9062 - val_loss: 2.5062\n",
      "Epoch 13/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 1.6370 - val_loss: 2.3135\n",
      "Epoch 14/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 1.3823 - val_loss: 2.1256\n",
      "Epoch 15/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 1.1628 - val_loss: 1.9495\n",
      "Epoch 16/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.9771 - val_loss: 1.7861\n",
      "Epoch 17/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8329 - val_loss: 1.6395\n",
      "Epoch 18/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.7098 - val_loss: 1.5125\n",
      "Epoch 19/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.6472 - val_loss: 1.4038\n",
      "Epoch 20/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5888 - val_loss: 1.3166\n",
      "Epoch 21/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5001 - val_loss: 1.2476\n",
      "Epoch 22/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5158 - val_loss: 1.1844\n",
      "Epoch 23/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4742 - val_loss: 1.1414\n",
      "Epoch 24/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4517 - val_loss: 1.1059\n",
      "Epoch 25/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4582 - val_loss: 1.0795\n",
      "Epoch 26/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4323 - val_loss: 1.0635\n",
      "Epoch 27/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4897 - val_loss: 1.0393\n",
      "Epoch 28/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4503 - val_loss: 1.0238\n",
      "Epoch 29/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4367 - val_loss: 1.0158\n",
      "Epoch 30/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4587 - val_loss: 1.0067\n",
      "Epoch 31/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4623 - val_loss: 1.0004\n",
      "Epoch 32/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4767 - val_loss: 0.9971\n",
      "Epoch 33/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4348 - val_loss: 0.9927\n",
      "Epoch 34/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4654 - val_loss: 0.9968\n",
      "Epoch 35/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4605 - val_loss: 0.9898\n",
      "Epoch 36/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4754 - val_loss: 0.9916\n",
      "Epoch 37/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4650 - val_loss: 0.9964\n",
      "Epoch 38/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4502 - val_loss: 0.9981\n",
      "Epoch 39/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4637 - val_loss: 0.9994\n",
      "Epoch 40/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4536 - val_loss: 0.9994\n",
      "Epoch 41/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4607 - val_loss: 0.9988\n",
      "Epoch 42/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4546 - val_loss: 0.9951\n",
      "Epoch 43/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4829 - val_loss: 0.9914\n",
      "Epoch 44/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4499 - val_loss: 0.9927\n",
      "Epoch 45/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4475 - val_loss: 0.9913\n",
      "Epoch 46/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4955 - val_loss: 1.0024\n",
      "Epoch 47/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4745 - val_loss: 1.0022\n",
      "Epoch 48/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4711 - val_loss: 1.0081\n",
      "Epoch 49/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4587 - val_loss: 1.0056\n",
      "Epoch 50/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4814 - val_loss: 0.9970\n",
      "Epoch 51/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4378 - val_loss: 0.9952\n",
      "Epoch 52/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4849 - val_loss: 0.9945\n",
      "Epoch 53/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4614 - val_loss: 0.9954\n",
      "Epoch 54/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4526 - val_loss: 0.9915\n",
      "Epoch 55/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4609 - val_loss: 0.9884\n",
      "Epoch 56/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4576 - val_loss: 0.9852\n",
      "Epoch 57/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4731 - val_loss: 0.9839\n",
      "Epoch 58/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4820 - val_loss: 0.9815\n",
      "Epoch 59/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4700 - val_loss: 0.9877\n",
      "Epoch 60/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4931 - val_loss: 0.9831\n",
      "Epoch 61/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4614 - val_loss: 0.9799\n",
      "Epoch 62/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4552 - val_loss: 0.9831\n",
      "Epoch 63/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4374 - val_loss: 0.9868\n",
      "Epoch 64/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4365 - val_loss: 0.9807\n",
      "Epoch 65/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4392 - val_loss: 0.9885\n",
      "Epoch 66/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4697 - val_loss: 0.9955\n",
      "Epoch 67/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4510 - val_loss: 0.9883\n",
      "Epoch 68/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4685 - val_loss: 0.9956\n",
      "Epoch 69/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4635 - val_loss: 1.0038\n",
      "Epoch 70/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4751 - val_loss: 0.9988\n",
      "Epoch 71/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4931 - val_loss: 1.0026\n",
      "Epoch 72/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4638 - val_loss: 0.9924\n",
      "Epoch 73/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4820 - val_loss: 0.9891\n",
      "Epoch 74/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4756 - val_loss: 0.9855\n",
      "Epoch 75/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4600 - val_loss: 0.9788\n",
      "Epoch 76/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4624 - val_loss: 0.9782\n",
      "Epoch 77/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4752 - val_loss: 0.9844\n",
      "Epoch 78/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4510 - val_loss: 0.9838\n",
      "Epoch 79/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4548 - val_loss: 0.9856\n",
      "Epoch 80/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4761 - val_loss: 0.9807\n",
      "Epoch 81/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4751 - val_loss: 0.9780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4523 - val_loss: 0.9813\n",
      "Epoch 83/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4598 - val_loss: 0.9821\n",
      "Epoch 84/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4720 - val_loss: 0.9791\n",
      "Epoch 85/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4473 - val_loss: 0.9772\n",
      "Epoch 86/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4649 - val_loss: 0.9698\n",
      "Epoch 87/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4502 - val_loss: 0.9722\n",
      "Epoch 88/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4831 - val_loss: 0.9763\n",
      "Epoch 89/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4576 - val_loss: 0.9718\n",
      "Epoch 90/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4678 - val_loss: 0.9742\n",
      "Epoch 91/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4640 - val_loss: 0.9817\n",
      "Epoch 92/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4551 - val_loss: 0.9869\n",
      "Epoch 93/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4492 - val_loss: 0.9856\n",
      "Epoch 94/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4843 - val_loss: 0.9911\n",
      "Epoch 95/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4568 - val_loss: 0.9924\n",
      "Epoch 96/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4768 - val_loss: 1.0017\n",
      "Epoch 97/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4466 - val_loss: 0.9907\n",
      "Epoch 98/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4698 - val_loss: 0.9850\n",
      "Epoch 99/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4565 - val_loss: 0.9741\n",
      "Epoch 100/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4872 - val_loss: 0.9775\n",
      "Epoch 101/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4525 - val_loss: 0.9774\n",
      "Epoch 102/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4580 - val_loss: 0.9849\n",
      "Epoch 103/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4787 - val_loss: 0.9919\n",
      "Epoch 104/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4634 - val_loss: 1.0001\n",
      "Epoch 105/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4739 - val_loss: 0.9928\n",
      "Epoch 106/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4656 - val_loss: 0.9849\n",
      "Epoch 107/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4951 - val_loss: 0.9772\n",
      "Epoch 108/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4335 - val_loss: 0.9766\n",
      "Epoch 109/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4421 - val_loss: 0.9774\n",
      "Epoch 110/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4690 - val_loss: 0.9687\n",
      "Epoch 111/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4376 - val_loss: 0.9752\n",
      "Epoch 112/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4386 - val_loss: 0.9754\n",
      "Epoch 113/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4677 - val_loss: 0.9819\n",
      "Epoch 114/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4495 - val_loss: 0.9813\n",
      "Epoch 115/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4728 - val_loss: 0.9814\n",
      "Epoch 116/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4479 - val_loss: 0.9751\n",
      "Epoch 117/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4589 - val_loss: 0.9697\n",
      "Epoch 118/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4810 - val_loss: 0.9702\n",
      "Epoch 119/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4776 - val_loss: 0.9660\n",
      "Epoch 120/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4751 - val_loss: 0.9710\n",
      "Epoch 121/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4808 - val_loss: 0.9750\n",
      "Epoch 122/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4507 - val_loss: 0.9789\n",
      "Epoch 123/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4602 - val_loss: 0.9801\n",
      "Epoch 124/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4680 - val_loss: 0.9746\n",
      "Epoch 125/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4599 - val_loss: 0.9763\n",
      "Epoch 126/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4672 - val_loss: 0.9740\n",
      "Epoch 127/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4469 - val_loss: 0.9689\n",
      "Epoch 128/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4438 - val_loss: 0.9726\n",
      "Epoch 129/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4613 - val_loss: 0.9746\n",
      "Epoch 130/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4725 - val_loss: 0.9835\n",
      "Epoch 131/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4674 - val_loss: 0.9844\n",
      "Epoch 132/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4524 - val_loss: 0.9715\n",
      "Epoch 133/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5028 - val_loss: 0.9747\n",
      "Epoch 134/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4474 - val_loss: 0.9707\n",
      "Epoch 135/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4694 - val_loss: 0.9697\n",
      "Epoch 136/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4748 - val_loss: 0.9617\n",
      "Epoch 137/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4473 - val_loss: 0.9624\n",
      "Epoch 138/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4331 - val_loss: 0.9672\n",
      "Epoch 139/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4662 - val_loss: 0.9685\n",
      "Epoch 140/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4407 - val_loss: 0.9664\n",
      "Epoch 141/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4594 - val_loss: 0.9746\n",
      "Epoch 142/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4469 - val_loss: 0.9637\n",
      "Epoch 143/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4582 - val_loss: 0.9749\n",
      "Epoch 144/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4671 - val_loss: 0.9683\n",
      "Epoch 145/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4629 - val_loss: 0.9637\n",
      "Epoch 146/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4566 - val_loss: 0.9656\n",
      "Epoch 147/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4553 - val_loss: 0.9674\n",
      "Epoch 148/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4754 - val_loss: 0.9598\n",
      "Epoch 149/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4652 - val_loss: 0.9692\n",
      "Epoch 150/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4605 - val_loss: 0.9570\n",
      "Epoch 151/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4710 - val_loss: 0.9628\n",
      "Epoch 152/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4488 - val_loss: 0.9581\n",
      "Epoch 153/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4836 - val_loss: 0.9718\n",
      "Epoch 154/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4829 - val_loss: 0.9643\n",
      "Epoch 155/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4684 - val_loss: 0.9711\n",
      "Epoch 156/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4622 - val_loss: 0.9708\n",
      "Epoch 157/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4866 - val_loss: 0.9640\n",
      "Epoch 158/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4474 - val_loss: 0.9579\n",
      "Epoch 159/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4478 - val_loss: 0.9494\n",
      "Epoch 160/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4421 - val_loss: 0.9587\n",
      "Epoch 161/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4458 - val_loss: 0.9585\n",
      "Epoch 162/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4596 - val_loss: 0.9585\n",
      "Epoch 163/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4593 - val_loss: 0.9596\n",
      "Epoch 164/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4716 - val_loss: 0.9638\n",
      "Epoch 165/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4257 - val_loss: 0.9586\n",
      "Epoch 166/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4755 - val_loss: 0.9548\n",
      "Epoch 167/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4915 - val_loss: 0.9587\n",
      "Epoch 168/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4753 - val_loss: 0.9527\n",
      "Epoch 169/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4802 - val_loss: 0.9494\n",
      "Epoch 170/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4505 - val_loss: 0.9620\n",
      "Epoch 171/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4721 - val_loss: 0.9648\n",
      "Epoch 172/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4734 - val_loss: 0.9676\n",
      "Epoch 173/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4545 - val_loss: 0.9654\n",
      "Epoch 174/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4561 - val_loss: 0.9686\n",
      "Epoch 175/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4811 - val_loss: 0.9623\n",
      "Epoch 176/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4618 - val_loss: 0.9637\n",
      "Epoch 177/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4678 - val_loss: 0.9549\n",
      "Epoch 178/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4520 - val_loss: 0.9514\n",
      "Epoch 179/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4580 - val_loss: 0.9596\n",
      "Epoch 180/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4615 - val_loss: 0.9537\n",
      "Epoch 181/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4753 - val_loss: 0.9538\n",
      "Epoch 182/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4250 - val_loss: 0.9550\n",
      "Epoch 183/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4678 - val_loss: 0.9582\n",
      "Epoch 184/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4656 - val_loss: 0.9694\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "Epoch 185/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4638 - val_loss: 0.9674\n",
      "Epoch 186/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4715 - val_loss: 0.9613\n",
      "Epoch 187/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4727 - val_loss: 0.9616\n",
      "Epoch 188/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4503 - val_loss: 0.9618\n",
      "Epoch 189/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4861 - val_loss: 0.9592\n",
      "Epoch 190/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4602 - val_loss: 0.9595\n",
      "Epoch 191/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4552 - val_loss: 0.9593\n",
      "Epoch 192/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4521 - val_loss: 0.9573\n",
      "Epoch 193/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4571 - val_loss: 0.9595\n",
      "Epoch 194/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4720 - val_loss: 0.9617\n",
      "Epoch 195/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4625 - val_loss: 0.9583\n",
      "Epoch 196/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4612 - val_loss: 0.9523\n",
      "Epoch 197/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4708 - val_loss: 0.9537\n",
      "Epoch 198/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4718 - val_loss: 0.9495\n",
      "Epoch 199/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4566 - val_loss: 0.9494\n",
      "Epoch 200/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4427 - val_loss: 0.9481\n",
      "Epoch 201/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4842 - val_loss: 0.9539\n",
      "Epoch 202/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4909 - val_loss: 0.9568\n",
      "Epoch 203/600\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4464 - val_loss: 0.9562\n",
      "Epoch 204/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4535 - val_loss: 0.9566\n",
      "Epoch 205/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4921 - val_loss: 0.9539\n",
      "Epoch 206/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4429 - val_loss: 0.9537\n",
      "Epoch 207/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4558 - val_loss: 0.9537\n",
      "Epoch 208/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4557 - val_loss: 0.9501\n",
      "Epoch 209/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4856 - val_loss: 0.9491\n",
      "Epoch 210/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4520 - val_loss: 0.9508\n",
      "Epoch 211/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4576 - val_loss: 0.9519\n",
      "Epoch 212/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4853 - val_loss: 0.9478\n",
      "Epoch 213/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4393 - val_loss: 0.9469\n",
      "Epoch 214/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4582 - val_loss: 0.9466\n",
      "Epoch 215/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4893 - val_loss: 0.9472\n",
      "Epoch 216/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5005 - val_loss: 0.9476\n",
      "Epoch 217/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4710 - val_loss: 0.9489\n",
      "Epoch 218/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4491 - val_loss: 0.9491\n",
      "Epoch 219/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4565 - val_loss: 0.9486\n",
      "Epoch 220/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4510 - val_loss: 0.9480\n",
      "Epoch 221/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4678 - val_loss: 0.9471\n",
      "Epoch 222/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4431 - val_loss: 0.9466\n",
      "Epoch 223/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4781 - val_loss: 0.9448\n",
      "Epoch 224/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4489 - val_loss: 0.9423\n",
      "Epoch 225/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4502 - val_loss: 0.9418\n",
      "Epoch 226/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4464 - val_loss: 0.9433\n",
      "Epoch 227/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4191 - val_loss: 0.9412\n",
      "Epoch 228/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4770 - val_loss: 0.9424\n",
      "Epoch 229/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4359 - val_loss: 0.9399\n",
      "Epoch 230/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4670 - val_loss: 0.9419\n",
      "Epoch 231/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4555 - val_loss: 0.9437\n",
      "Epoch 232/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4580 - val_loss: 0.9427\n",
      "Epoch 233/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4865 - val_loss: 0.9403\n",
      "Epoch 234/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4498 - val_loss: 0.9419\n",
      "Epoch 235/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4538 - val_loss: 0.9409\n",
      "Epoch 236/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4553 - val_loss: 0.9434\n",
      "Epoch 237/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4330 - val_loss: 0.9469\n",
      "Epoch 238/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4870 - val_loss: 0.9461\n",
      "Epoch 239/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4521 - val_loss: 0.9437\n",
      "Epoch 240/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4519 - val_loss: 0.9481\n",
      "Epoch 241/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4448 - val_loss: 0.9491\n",
      "Epoch 242/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4546 - val_loss: 0.9480\n",
      "Epoch 243/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4600 - val_loss: 0.9459\n",
      "Epoch 244/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4557 - val_loss: 0.9463\n",
      "Epoch 245/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4959 - val_loss: 0.9488\n",
      "Epoch 246/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4475 - val_loss: 0.9435\n",
      "Epoch 247/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4546 - val_loss: 0.9473\n",
      "Epoch 248/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4616 - val_loss: 0.9456\n",
      "Epoch 249/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4637 - val_loss: 0.9431\n",
      "Epoch 250/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4661 - val_loss: 0.9407\n",
      "Epoch 251/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4880 - val_loss: 0.9430\n",
      "Epoch 252/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4844 - val_loss: 0.9433\n",
      "Epoch 253/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4408 - val_loss: 0.9422\n",
      "Epoch 254/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4446 - val_loss: 0.9422\n",
      "\n",
      "Epoch 00254: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 255/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4519 - val_loss: 0.9413\n",
      "Epoch 256/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4617 - val_loss: 0.9416\n",
      "Epoch 257/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4520 - val_loss: 0.9428\n",
      "Epoch 258/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4816 - val_loss: 0.9425\n",
      "Epoch 259/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4512 - val_loss: 0.9429\n",
      "Epoch 260/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4460 - val_loss: 0.9421\n",
      "Epoch 261/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4590 - val_loss: 0.9417\n",
      "Epoch 262/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4519 - val_loss: 0.9419\n",
      "Epoch 263/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4465 - val_loss: 0.9420\n",
      "Epoch 264/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4696 - val_loss: 0.9427\n",
      "Epoch 265/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4768 - val_loss: 0.9445\n",
      "Epoch 266/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4701 - val_loss: 0.9448\n",
      "Epoch 267/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4289 - val_loss: 0.9464\n",
      "Epoch 268/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4479 - val_loss: 0.9459\n",
      "Epoch 269/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4477 - val_loss: 0.9451\n",
      "Epoch 270/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4708 - val_loss: 0.9449\n",
      "Epoch 271/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4577 - val_loss: 0.9463\n",
      "Epoch 272/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4596 - val_loss: 0.9476\n",
      "Epoch 273/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4800 - val_loss: 0.9490\n",
      "Epoch 274/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4300 - val_loss: 0.9500\n",
      "Epoch 275/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4755 - val_loss: 0.9513\n",
      "Epoch 276/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4838 - val_loss: 0.9506\n",
      "Epoch 277/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4365 - val_loss: 0.9490\n",
      "Epoch 278/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4579 - val_loss: 0.9496\n",
      "Epoch 279/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4853 - val_loss: 0.9507\n",
      "\n",
      "Epoch 00279: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 280/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4758 - val_loss: 0.9498\n",
      "Epoch 281/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4751 - val_loss: 0.9486\n",
      "Epoch 282/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4669 - val_loss: 0.9476\n",
      "Epoch 283/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4505 - val_loss: 0.9467\n",
      "Epoch 284/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4646 - val_loss: 0.9466\n",
      "Epoch 285/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4517 - val_loss: 0.9463\n",
      "Epoch 286/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4478 - val_loss: 0.9460\n",
      "Epoch 287/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4468 - val_loss: 0.9457\n",
      "Epoch 288/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4257 - val_loss: 0.9458\n",
      "Epoch 289/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4336 - val_loss: 0.9452\n",
      "Epoch 290/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4499 - val_loss: 0.9448\n",
      "Epoch 291/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4468 - val_loss: 0.9438\n",
      "Epoch 292/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4413 - val_loss: 0.9435\n",
      "Epoch 293/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4527 - val_loss: 0.9432\n",
      "Epoch 294/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4615 - val_loss: 0.9433\n",
      "Epoch 295/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4341 - val_loss: 0.9439\n",
      "Epoch 296/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4622 - val_loss: 0.9433\n",
      "Epoch 297/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4697 - val_loss: 0.9426\n",
      "Epoch 298/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4830 - val_loss: 0.9431\n",
      "Epoch 299/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4523 - val_loss: 0.9429\n",
      "Epoch 300/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4604 - val_loss: 0.9416\n",
      "Epoch 301/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4427 - val_loss: 0.9416\n",
      "Epoch 302/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4540 - val_loss: 0.9411\n",
      "Epoch 303/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4600 - val_loss: 0.9415\n",
      "Epoch 304/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4626 - val_loss: 0.9410\n",
      "\n",
      "Epoch 00304: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 305/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4646 - val_loss: 0.9409\n",
      "Epoch 306/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4818 - val_loss: 0.9415\n",
      "Epoch 307/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4660 - val_loss: 0.9410\n",
      "Epoch 308/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4620 - val_loss: 0.9406\n",
      "Epoch 309/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4579 - val_loss: 0.9409\n",
      "Epoch 310/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4480 - val_loss: 0.9409\n",
      "Epoch 311/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4608 - val_loss: 0.9403\n",
      "Epoch 312/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4364 - val_loss: 0.9402\n",
      "Epoch 313/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4723 - val_loss: 0.9400\n",
      "Epoch 314/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4593 - val_loss: 0.9402\n",
      "Epoch 315/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4657 - val_loss: 0.9400\n",
      "Epoch 316/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4788 - val_loss: 0.9400\n",
      "Epoch 317/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4445 - val_loss: 0.9401\n",
      "Epoch 318/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4398 - val_loss: 0.9403\n",
      "Epoch 319/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4868 - val_loss: 0.9404\n",
      "Epoch 320/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4524 - val_loss: 0.9408\n",
      "Epoch 321/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4520 - val_loss: 0.9410\n",
      "Epoch 322/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4674 - val_loss: 0.9408\n",
      "Epoch 323/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4434 - val_loss: 0.9406\n",
      "Epoch 324/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4738 - val_loss: 0.9407\n",
      "Epoch 325/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4665 - val_loss: 0.9407\n",
      "Epoch 326/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4650 - val_loss: 0.9403\n",
      "Epoch 327/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4521 - val_loss: 0.9398\n",
      "Epoch 328/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4179 - val_loss: 0.9400\n",
      "Epoch 329/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4352 - val_loss: 0.9404\n",
      "Epoch 330/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4655 - val_loss: 0.9401\n",
      "Epoch 331/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4753 - val_loss: 0.9406\n",
      "Epoch 332/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4866 - val_loss: 0.9406\n",
      "Epoch 333/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4515 - val_loss: 0.9406\n",
      "Epoch 334/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4539 - val_loss: 0.9405\n",
      "Epoch 335/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4547 - val_loss: 0.9407\n",
      "Epoch 336/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4742 - val_loss: 0.9404\n",
      "Epoch 337/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4645 - val_loss: 0.9404\n",
      "Epoch 338/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4703 - val_loss: 0.9403\n",
      "Epoch 339/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4636 - val_loss: 0.9406\n",
      "Epoch 340/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4572 - val_loss: 0.9407\n",
      "Epoch 341/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4630 - val_loss: 0.9409\n",
      "Epoch 342/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4639 - val_loss: 0.9408\n",
      "Epoch 343/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4585 - val_loss: 0.9405\n",
      "Epoch 344/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4659 - val_loss: 0.9405\n",
      "Epoch 345/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4566 - val_loss: 0.9408\n",
      "Epoch 346/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4578 - val_loss: 0.9404\n",
      "Epoch 347/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4543 - val_loss: 0.9408\n",
      "Epoch 348/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4611 - val_loss: 0.9410\n",
      "Epoch 349/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4610 - val_loss: 0.9413\n",
      "Epoch 350/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4592 - val_loss: 0.9416\n",
      "Epoch 351/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4666 - val_loss: 0.9414\n",
      "Epoch 352/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4375 - val_loss: 0.9413\n",
      "\n",
      "Epoch 00352: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 353/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4652 - val_loss: 0.9410\n",
      "Epoch 354/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4821 - val_loss: 0.9409\n",
      "Epoch 355/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4432 - val_loss: 0.9407\n",
      "Epoch 356/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4600 - val_loss: 0.9406\n",
      "Epoch 357/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4491 - val_loss: 0.9407\n",
      "Epoch 358/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4457 - val_loss: 0.9407\n",
      "Epoch 359/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4499 - val_loss: 0.9407\n",
      "Epoch 360/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4645 - val_loss: 0.9407\n",
      "Epoch 361/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4723 - val_loss: 0.9408\n",
      "Epoch 362/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4560 - val_loss: 0.9408\n",
      "Epoch 363/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4623 - val_loss: 0.9406\n",
      "Epoch 364/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4614 - val_loss: 0.9407\n",
      "Epoch 365/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4526 - val_loss: 0.9408\n",
      "Epoch 366/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4566 - val_loss: 0.9407\n",
      "Epoch 367/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4498 - val_loss: 0.9408\n",
      "Epoch 368/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4666 - val_loss: 0.9410\n",
      "Epoch 369/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4534 - val_loss: 0.9410\n",
      "Epoch 370/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4808 - val_loss: 0.9410\n",
      "Epoch 371/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4660 - val_loss: 0.9409\n",
      "Epoch 372/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4688 - val_loss: 0.9408\n",
      "Epoch 373/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4400 - val_loss: 0.9408\n",
      "Epoch 374/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4539 - val_loss: 0.9407\n",
      "Epoch 375/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4284 - val_loss: 0.9407\n",
      "Epoch 376/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4288 - val_loss: 0.9408\n",
      "Epoch 377/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4713 - val_loss: 0.9409\n",
      "\n",
      "Epoch 00377: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 378/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4589 - val_loss: 0.9409\n",
      "Epoch 379/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4459 - val_loss: 0.9409\n",
      "Epoch 380/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4661 - val_loss: 0.9410\n",
      "Epoch 381/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4653 - val_loss: 0.9409\n",
      "Epoch 382/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4426 - val_loss: 0.9409\n",
      "Epoch 383/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4546 - val_loss: 0.9408\n",
      "Epoch 384/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4880 - val_loss: 0.9408\n",
      "Epoch 385/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4701 - val_loss: 0.9408\n",
      "Epoch 386/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4497 - val_loss: 0.9409\n",
      "Epoch 387/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4391 - val_loss: 0.9409\n",
      "Epoch 388/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4624 - val_loss: 0.9409\n",
      "Epoch 389/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4557 - val_loss: 0.9408\n",
      "Epoch 390/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4605 - val_loss: 0.9407\n",
      "Epoch 391/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4646 - val_loss: 0.9407\n",
      "Epoch 392/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4803 - val_loss: 0.9407\n",
      "Epoch 393/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4668 - val_loss: 0.9408\n",
      "Epoch 394/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4311 - val_loss: 0.9409\n",
      "Epoch 395/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4507 - val_loss: 0.9410\n",
      "Epoch 396/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4600 - val_loss: 0.9410\n",
      "Epoch 397/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4737 - val_loss: 0.9410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4581 - val_loss: 0.9410\n",
      "Epoch 399/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4360 - val_loss: 0.9409\n",
      "Epoch 400/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4621 - val_loss: 0.9410\n",
      "Epoch 401/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4345 - val_loss: 0.9409\n",
      "Epoch 402/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4448 - val_loss: 0.9410\n",
      "\n",
      "Epoch 00402: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Epoch 403/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4702 - val_loss: 0.9410\n",
      "Epoch 404/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4733 - val_loss: 0.9409\n",
      "Epoch 405/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4724 - val_loss: 0.9410\n",
      "Epoch 406/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4365 - val_loss: 0.9410\n",
      "Epoch 407/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4530 - val_loss: 0.9409\n",
      "Epoch 408/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4493 - val_loss: 0.9409\n",
      "Epoch 409/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4468 - val_loss: 0.9410\n",
      "Epoch 410/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4608 - val_loss: 0.9409\n",
      "Epoch 411/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4474 - val_loss: 0.9409\n",
      "Epoch 412/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4604 - val_loss: 0.9409\n",
      "Epoch 413/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4500 - val_loss: 0.9409\n",
      "Epoch 414/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4571 - val_loss: 0.9408\n",
      "Epoch 415/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4538 - val_loss: 0.9408\n",
      "Epoch 416/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4816 - val_loss: 0.9409\n",
      "Epoch 417/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4568 - val_loss: 0.9408\n",
      "Epoch 418/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4549 - val_loss: 0.9408\n",
      "Epoch 419/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4512 - val_loss: 0.9408\n",
      "Epoch 420/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4629 - val_loss: 0.9408\n",
      "Epoch 421/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4526 - val_loss: 0.9408\n",
      "Epoch 422/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4440 - val_loss: 0.9408\n",
      "Epoch 423/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4663 - val_loss: 0.9408\n",
      "Epoch 424/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4536 - val_loss: 0.9408\n",
      "Epoch 425/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4775 - val_loss: 0.9408\n",
      "Epoch 426/600\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4678 - val_loss: 0.9408\n",
      "Epoch 427/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4537 - val_loss: 0.9408\n",
      "\n",
      "Epoch 00427: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 428/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4613 - val_loss: 0.9408\n",
      "Epoch 429/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4401 - val_loss: 0.9408\n",
      "Epoch 430/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4579 - val_loss: 0.9408\n",
      "Epoch 431/600\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.4504 - val_loss: 0.9408\n",
      "Epoch 432/600\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.4971 - val_loss: 0.9408\n",
      "Epoch 433/600\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.4821 - val_loss: 0.9408\n",
      "Epoch 434/600\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.4675 - val_loss: 0.9408\n",
      "Epoch 435/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4462 - val_loss: 0.9408\n",
      "Epoch 436/600\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.4639 - val_loss: 0.9408\n",
      "Epoch 437/600\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.4710 - val_loss: 0.9408\n",
      "Epoch 438/600\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.4835 - val_loss: 0.9408\n",
      "Epoch 439/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4846 - val_loss: 0.9408\n",
      "Epoch 440/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4614 - val_loss: 0.9408\n",
      "Epoch 441/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4635 - val_loss: 0.9408\n",
      "Epoch 442/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4723 - val_loss: 0.9408\n",
      "Epoch 443/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4487 - val_loss: 0.9408\n",
      "Epoch 444/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4672 - val_loss: 0.9409\n",
      "Epoch 445/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4750 - val_loss: 0.9409\n",
      "Epoch 446/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4635 - val_loss: 0.9408\n",
      "Epoch 447/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4656 - val_loss: 0.9409\n",
      "Epoch 448/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4668 - val_loss: 0.9408\n",
      "Epoch 449/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4340 - val_loss: 0.9408\n",
      "Epoch 450/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4590 - val_loss: 0.9408\n",
      "Epoch 451/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4641 - val_loss: 0.9408\n",
      "Epoch 452/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4823 - val_loss: 0.9408\n",
      "\n",
      "Epoch 00452: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "Epoch 453/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4621 - val_loss: 0.9408\n",
      "Epoch 454/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4720 - val_loss: 0.9408\n",
      "Epoch 455/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4478 - val_loss: 0.9408\n",
      "Epoch 456/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4657 - val_loss: 0.9408\n",
      "Epoch 457/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4502 - val_loss: 0.9408\n",
      "Epoch 458/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4704 - val_loss: 0.9408\n",
      "Epoch 459/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4455 - val_loss: 0.9408\n",
      "Epoch 460/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4893 - val_loss: 0.9408\n",
      "Epoch 461/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4593 - val_loss: 0.9408\n",
      "Epoch 462/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4549 - val_loss: 0.9408\n",
      "Epoch 463/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4249 - val_loss: 0.9408\n",
      "Epoch 464/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4682 - val_loss: 0.9408\n",
      "Epoch 465/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4737 - val_loss: 0.9408\n",
      "Epoch 466/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4358 - val_loss: 0.9408\n",
      "Epoch 467/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4163 - val_loss: 0.9408\n",
      "Epoch 468/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4545 - val_loss: 0.9407\n",
      "Epoch 469/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4449 - val_loss: 0.9407\n",
      "Epoch 470/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4651 - val_loss: 0.9407\n",
      "Epoch 471/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4469 - val_loss: 0.9407\n",
      "Epoch 472/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4441 - val_loss: 0.9407\n",
      "Epoch 473/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4330 - val_loss: 0.9407\n",
      "Epoch 474/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4354 - val_loss: 0.9407\n",
      "Epoch 475/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4587 - val_loss: 0.9407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 476/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4652 - val_loss: 0.9407\n",
      "Epoch 477/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4611 - val_loss: 0.9407\n",
      "\n",
      "Epoch 00477: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 478/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4426 - val_loss: 0.9407\n",
      "Epoch 479/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4464 - val_loss: 0.9407\n",
      "Epoch 480/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4428 - val_loss: 0.9407\n",
      "Epoch 481/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4639 - val_loss: 0.9407\n",
      "Epoch 482/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4496 - val_loss: 0.9407\n",
      "Epoch 483/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4644 - val_loss: 0.9407\n",
      "Epoch 484/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4460 - val_loss: 0.9407\n",
      "Epoch 485/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4579 - val_loss: 0.9407\n",
      "Epoch 486/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4691 - val_loss: 0.9407\n",
      "Epoch 487/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4583 - val_loss: 0.9407\n",
      "Epoch 488/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4290 - val_loss: 0.9407\n",
      "Epoch 489/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4549 - val_loss: 0.9407\n",
      "Epoch 490/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4476 - val_loss: 0.9407\n",
      "Epoch 491/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4405 - val_loss: 0.9407\n",
      "Epoch 492/600\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4637 - val_loss: 0.9407\n",
      "Epoch 493/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4555 - val_loss: 0.9407\n",
      "Epoch 494/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4687 - val_loss: 0.9407\n",
      "Epoch 495/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4390 - val_loss: 0.9407\n",
      "Epoch 496/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4508 - val_loss: 0.9407\n",
      "Epoch 497/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4537 - val_loss: 0.9407\n",
      "Epoch 498/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4837 - val_loss: 0.9407\n",
      "Epoch 499/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4720 - val_loss: 0.9407\n",
      "Epoch 500/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4385 - val_loss: 0.9407\n",
      "Epoch 501/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4610 - val_loss: 0.9407\n",
      "Epoch 502/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4579 - val_loss: 0.9407\n",
      "\n",
      "Epoch 00502: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "Epoch 503/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4469 - val_loss: 0.9407\n",
      "Epoch 504/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4503 - val_loss: 0.9407\n",
      "Epoch 505/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4623 - val_loss: 0.9407\n",
      "Epoch 506/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4918 - val_loss: 0.9407\n",
      "Epoch 507/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4486 - val_loss: 0.9407\n",
      "Epoch 508/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4581 - val_loss: 0.9407\n",
      "Epoch 509/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4644 - val_loss: 0.9407\n",
      "Epoch 510/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4709 - val_loss: 0.9407\n",
      "Epoch 511/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4249 - val_loss: 0.9407\n",
      "Epoch 512/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4512 - val_loss: 0.9407\n",
      "Epoch 513/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4615 - val_loss: 0.9407\n",
      "Epoch 514/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4910 - val_loss: 0.9407\n",
      "Epoch 515/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4557 - val_loss: 0.9407\n",
      "Epoch 516/600\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.4551 - val_loss: 0.9407\n",
      "Epoch 517/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4773 - val_loss: 0.9407\n",
      "Epoch 518/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4502 - val_loss: 0.9407\n",
      "Epoch 519/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4504 - val_loss: 0.9407\n",
      "Epoch 520/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4750 - val_loss: 0.9407\n",
      "Epoch 521/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4509 - val_loss: 0.9407\n",
      "Epoch 522/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4721 - val_loss: 0.9407\n",
      "Epoch 523/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4564 - val_loss: 0.9407\n",
      "Epoch 524/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4698 - val_loss: 0.9407\n",
      "Epoch 525/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4628 - val_loss: 0.9407\n",
      "Epoch 526/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4714 - val_loss: 0.9407\n",
      "Epoch 527/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4696 - val_loss: 0.9407\n",
      "\n",
      "Epoch 00527: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 528/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4375 - val_loss: 0.9407\n",
      "Epoch 529/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4681 - val_loss: 0.9407\n",
      "Epoch 530/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4577 - val_loss: 0.9407\n",
      "Epoch 531/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4739 - val_loss: 0.9407\n",
      "Epoch 532/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4563 - val_loss: 0.9407\n",
      "Epoch 533/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4663 - val_loss: 0.9407\n",
      "Epoch 534/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4927 - val_loss: 0.9407\n",
      "Epoch 535/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4645 - val_loss: 0.9407\n",
      "Epoch 536/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4555 - val_loss: 0.9407\n",
      "Epoch 537/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4725 - val_loss: 0.9407\n",
      "Epoch 538/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4471 - val_loss: 0.9407\n",
      "Epoch 539/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4534 - val_loss: 0.9407\n",
      "Epoch 540/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4413 - val_loss: 0.9407\n",
      "Epoch 541/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4635 - val_loss: 0.9407\n",
      "Epoch 542/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4635 - val_loss: 0.9407\n",
      "Epoch 543/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4506 - val_loss: 0.9407\n",
      "Epoch 544/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4535 - val_loss: 0.9407\n",
      "Epoch 545/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4673 - val_loss: 0.9407\n",
      "Epoch 546/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4914 - val_loss: 0.9407\n",
      "Epoch 547/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4589 - val_loss: 0.9407\n",
      "Epoch 548/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4768 - val_loss: 0.9407\n",
      "Epoch 549/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4813 - val_loss: 0.9407\n",
      "Epoch 550/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4561 - val_loss: 0.9407\n",
      "Epoch 551/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4587 - val_loss: 0.9407\n",
      "Epoch 552/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4316 - val_loss: 0.9407\n",
      "\n",
      "Epoch 00552: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "Epoch 553/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4679 - val_loss: 0.9407\n",
      "Epoch 554/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4322 - val_loss: 0.9407\n",
      "Epoch 555/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4520 - val_loss: 0.9407\n",
      "Epoch 556/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4655 - val_loss: 0.9407\n",
      "Epoch 557/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4621 - val_loss: 0.9407\n",
      "Epoch 558/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4593 - val_loss: 0.9407\n",
      "Epoch 559/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4664 - val_loss: 0.9407\n",
      "Epoch 560/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4482 - val_loss: 0.9407\n",
      "Epoch 561/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4490 - val_loss: 0.9407\n",
      "Epoch 562/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4656 - val_loss: 0.9407\n",
      "Epoch 563/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4361 - val_loss: 0.9407\n",
      "Epoch 564/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4510 - val_loss: 0.9407\n",
      "Epoch 565/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4591 - val_loss: 0.9407\n",
      "Epoch 566/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4518 - val_loss: 0.9407\n",
      "Epoch 567/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4555 - val_loss: 0.9407\n",
      "Epoch 568/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4366 - val_loss: 0.9407\n",
      "Epoch 569/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4509 - val_loss: 0.9407\n",
      "Epoch 570/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4694 - val_loss: 0.9407\n",
      "Epoch 571/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4696 - val_loss: 0.9407\n",
      "Epoch 572/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4681 - val_loss: 0.9407\n",
      "Epoch 573/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4605 - val_loss: 0.9407\n",
      "Epoch 574/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4414 - val_loss: 0.9407\n",
      "Epoch 575/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4422 - val_loss: 0.9407\n",
      "Epoch 576/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4780 - val_loss: 0.9407\n",
      "Epoch 577/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4724 - val_loss: 0.9407\n",
      "\n",
      "Epoch 00577: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 578/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4588 - val_loss: 0.9407\n",
      "Epoch 579/600\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4626 - val_loss: 0.9407\n",
      "Epoch 580/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4776 - val_loss: 0.9407\n",
      "Epoch 581/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4639 - val_loss: 0.9407\n",
      "Epoch 582/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4394 - val_loss: 0.9407\n",
      "Epoch 583/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4460 - val_loss: 0.9407\n",
      "Epoch 584/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4337 - val_loss: 0.9407\n",
      "Epoch 585/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4504 - val_loss: 0.9407\n",
      "Epoch 586/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4504 - val_loss: 0.9407\n",
      "Epoch 587/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4621 - val_loss: 0.9407\n",
      "Epoch 588/600\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4721 - val_loss: 0.9407\n",
      "Epoch 589/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4554 - val_loss: 0.9407\n",
      "Epoch 590/600\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4444 - val_loss: 0.9407\n",
      "Epoch 591/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4595 - val_loss: 0.9407\n",
      "Epoch 592/600\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4583 - val_loss: 0.9407\n",
      "Epoch 593/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4626 - val_loss: 0.9407\n",
      "Epoch 594/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4674 - val_loss: 0.9407\n",
      "Epoch 595/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4185 - val_loss: 0.9407\n",
      "Epoch 596/600\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4829 - val_loss: 0.9407\n",
      "Epoch 597/600\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4693 - val_loss: 0.9407\n",
      "Epoch 598/600\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4592 - val_loss: 0.9407\n",
      "Epoch 599/600\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4466 - val_loss: 0.9407\n",
      "Epoch 600/600\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4555 - val_loss: 0.9407\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rdlr = ReduceLROnPlateau(patience=25, factor=0.5, min_lr=1e-6, monitor='val_loss', verbose=1)\n",
    "\n",
    "h = model.fit(X_train, y_train, epochs=600, batch_size=8, validation_data=(X_test, y_test), callbacks=[rdlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import normalize, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "11  2.49\n",
       "7   2.27\n",
       "9   2.62\n",
       "16  2.39\n",
       "6   1.31\n",
       "1    1.8\n",
       "15   2.4\n",
       "12  2.53\n",
       "14  2.54\n",
       "13  2.48\n",
       "4   1.63\n",
       "10  2.46\n",
       "17  2.42\n",
       "5    1.5\n",
       "0   1.84\n",
       "19   4.4"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
